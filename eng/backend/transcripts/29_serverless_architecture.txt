Chapter 29: Serverless Architecture

Why Serverless?

Imagine your e-commerce site handles 1,000 requests per day during normal times. Then Black Friday arrives, and suddenly you're getting 50,000 requests per hour. With the traditional approach, you have two bad options. You could provision 50 servers in advance, which is expensive and they'll be mostly idle the rest of the year. Or you could underprovision, which means your site crashes and you lose sales.

The serverless approach offers a different trade-off. You pay only for the actual requests you receive, with automatic scaling from 1 to 1,000 concurrent executions, and zero capacity planning required.

But there's a catch. The first request to a cold function can take 3 seconds to respond. That means your checkout page just lost a customer.

Serverless isn't magic. It's a trade-off where you exchange operational complexity for cold start latency.

Function as a Service Fundamentals

The Problem

Traditional servers require capacity planning, patching, monitoring, and scaling orchestration. You spend more time managing infrastructure than writing business logic.

How It Works

In a serverless architecture, events flow from various sources like API Gateway, S3 buckets, SQS queues, or EventBridge to trigger function invocations. When a function is invoked, it can take two different paths.

The cold start path happens when there's no existing container available. The system has to download your code, start a container, initialize the runtime, load the code, and then finally execute your handler function. This entire cold start process can take anywhere from 500 milliseconds to 10 seconds.

The hot path is much faster. When a container already exists from a previous invocation, the system can simply use that existing container and execute the handler directly. This typically takes only 10 to 100 milliseconds.

Let me walk you through a basic Lambda function. The function handles order creation. At the top of the file, outside the handler function, you initialize resources that should be reused across invocations. For example, you create a DynamoDB resource and get a reference to the orders table. This initialization happens once per container, not once per request.

The lambda handler function is called for each invocation. It receives two parameters: the event, which contains request data from sources like API Gateway or S3, and the context, which provides runtime information like the request ID and memory limit.

Inside the handler, you first parse the input by loading the JSON body from the event and extracting the user ID. Then you execute your business logic, creating an order object with an order ID from the context, the user ID, a timestamp, and the items from the request body. You write this order to the database using the table's put item method. Finally, you return a response in the format expected by API Gateway, with a status code of 200, appropriate headers, and a JSON body containing the order ID.

For deployment, you configure the function with several properties. You specify the handler function, which is "order.lambda_handler". You choose the runtime, such as Python 3.11. You set the memory size to 512 megabytes. Remember that more memory means more CPU power. You define the timeout, which is the maximum execution time, such as 30 seconds. You set environment variables like the table name. And you configure events that trigger the function, such as an API Gateway endpoint for POST requests to the /orders path.

Trade-offs

Let's talk about the trade-offs of serverless functions.

For scaling, the pros are that it's automatic and instant. The cons are that you experience cold starts when scaling up.

For cost, the pro is that you pay per invocation. The con is that it becomes expensive at very high volume.

For operations, the pro is that you have no servers to manage. The con is that debugging and monitoring become harder.

For state management, the pro is that it forces you to design stateless applications. The con is that you can't keep an in-memory cache.

When to use serverless: Use it for event-driven workloads, unpredictable traffic patterns, and rapid prototyping.

When NOT to use serverless: Avoid it for long-running tasks that exceed 15 minutes, applications with consistent low latency requirements below 50 milliseconds, and high throughput workloads with more than 10 million requests per day, which may be cheaper on containers.

Cold Starts: The Hidden Tax

The Problem

When a function hasn't been invoked recently, the cloud provider must provision a new execution environment. This initialization overhead can add seconds to your response time.

Cold Start Anatomy

Let me break down what happens during a cold start for an AWS Lambda function running Python.

First, the system downloads your code package, which takes 100 to 500 milliseconds. Then it starts a micro-VM or container, taking 200 to 800 milliseconds. Next, it initializes the runtime environment, which takes 100 to 300 milliseconds. Then comes the biggest variable: importing dependencies. This can take anywhere from 50 milliseconds to 5 seconds depending on what libraries you use. After that, it executes your global scope code, which is the code outside your handler function. This takes 50 milliseconds to 2 seconds, and this is something you can control. Finally, it executes your handler function, which takes 10 milliseconds to 1 second.

In total, a cold start can range from 500 milliseconds to 10 seconds.

Mitigation Strategies

There are several strategies to mitigate cold starts.

Strategy 1: Minimize Package Size

The wrong approach is to import entire libraries when you only need a small part of them. For example, importing pandas, which is over 100 megabytes and takes 2 to 3 seconds to import, just to use a single function for date parsing is wasteful.

The better approach is to use lightweight alternatives. Instead of using pandas for date parsing, you can use Python's built-in datetime module, which imports almost instantly.

Another good approach is to use Lambda Layers for shared dependencies. Layers are stored at /opt/python/lib/ and are cached separately, leading to faster loading times.

Strategy 2: Lazy Initialization

The wrong approach is to initialize everything globally at the module level. You might create clients for DynamoDB, S3, and Secrets Manager all at once, even if a particular invocation only needs one of them.

The better approach is to initialize resources on-demand. You create global variables set to None, then write getter functions that create the client only when first called and cache it for subsequent calls. This way, you only pay for what you actually use. If a particular invocation doesn't need S3, the S3 client never gets initialized.

Strategy 3: Provisioned Concurrency

You can configure AWS Lambda to keep a certain number of instances always warm. You set a minimum provisioned concurrency, such as 5 instances, and a maximum, such as 100 instances. You also set a utilization target like 0.7 to control auto-scaling behavior.

The trade-off is that provisioned instances have no cold starts, but you pay for idle time just like with traditional servers. Use this only for latency-critical paths where cold starts would significantly impact user experience.

Strategy 4: Predictive Warming

You can schedule pre-warming before known traffic spikes. For example, if you know you get a daily traffic spike at 9 AM, you can run a warming function beforehand. This function invokes your critical API function multiple times, such as 10 invocations, with a special warmup flag in the payload.

Inside your function, you check for the warmup flag and simply return a status message if it's present. Otherwise, you proceed with normal logic.

Cold Start Comparison

Different runtimes have different cold start characteristics.

Python 3.11 typically has cold starts of 200 to 500 milliseconds. It's fast, but imports can add significant time.

Node.js 20 has cold starts of 150 to 400 milliseconds and is the fastest for small packages.

Java 17 has cold starts of 1 to 3 seconds due to JVM initialization overhead.

Go 1.x has cold starts of 100 to 300 milliseconds because it's compiled, making it very fast.

.NET 6 has cold starts of 500 milliseconds to 2 seconds, which is better than Java but worse than Python.

Runtime Memory Impact

Memory allocation also affects performance. With 128 megabytes of memory, you get slow CPU and long cold starts. With 256 megabytes, you get twice the speed at twice the cost. With 512 megabytes, you get four times the speed at four times the cost. With 1024 megabytes, you get eight times the speed at eight times the cost.

The price-performance sweet spot is usually between 512 and 1024 megabytes.

Event-Driven Patterns

S3 Triggered Processing

The Problem

You need to process uploaded files like images, videos, or CSV files without running a server that constantly polls for new files.

How It Works

The flow works like this: A user uploads an image to S3. This triggers an S3 event called "ObjectCreated" in the user-uploads bucket. This event invokes a Lambda function called "process-image". The function downloads the file from S3, resizes and compresses it, uploads it to a processed bucket, and updates the database. Finally, the processed image is stored in the processed-images bucket.

In the implementation, the Lambda function receives an event containing an array of records. For each record, you extract the bucket name and the object key from the S3 event structure. You download the original image using the S3 get object method and read the image data. You then use a library like PIL to open the image and create a thumbnail copy, resizing it to 300 by 300 pixels. You save the thumbnail to a buffer in the original format, then upload it to the processed images bucket with the appropriate content type.

API Gateway Integration

The Problem

You need a scalable REST API without managing load balancers, auto-scaling groups, or container orchestration.

How It Works

Here's the request flow: An HTTP request comes in, such as a POST to /users. It goes through API Gateway, which handles authentication using API keys or Cognito, performs request validation, applies rate limiting, and transforms the request if needed. The request is then forwarded to a Lambda function like "create-user", which receives the transformed event, executes business logic, and returns a structured response. API Gateway then handles response transformation, adds CORS headers, and optionally caches the response before returning the HTTP response to the client.

In the implementation, your Lambda handler receives an event that contains all the HTTP details. You extract the HTTP method, path, headers, query string parameters, and body. If you're using an API Gateway authorizer, you can access the authenticated user ID from the request context.

For a POST request to /users, you might create a new user object with a generated user ID, email, and name from the request body. You save this to your database, then return a response with a status code of 201 for created, appropriate headers including CORS headers, and a JSON body containing the new user data.

SQS Queue Processing

The Problem

You need to decouple producers and consumers, handle spiky workloads, and ensure at-least-once processing of messages.

How It Works

The architecture works like this: An order service acts as the producer and sends messages to an SQS queue called order-events. Lambda acts as the consumer and polls the queue every 5 seconds, processing messages in batches of up to 10. For each message, if processing succeeds, Lambda automatically deletes the message from the queue. If processing fails, the message is returned to the queue for retry. After 3 retries, the message is moved to a Dead Letter Queue for manual inspection.

In the implementation, your Lambda handler automatically receives batches of up to 10 messages in the event's Records array. For each record, you extract the message ID and parse the JSON body. You wrap the processing in a try-catch block. If processing succeeds, Lambda automatically deletes the message from the queue, so you just log success. If processing fails, you log the error and re-raise the exception. This tells Lambda that processing failed, and the message should be returned to the queue for retry.

For the queue configuration, you set the visibility timeout to 300 seconds, which should be longer than your Lambda timeout. You configure a redrive policy with a dead letter target ARN and a max receive count of 3, meaning after 3 failed attempts, messages go to the dead letter queue. For the Lambda function, you set the batch size to 10 to process 10 messages at once, and set maximum batching window to 5 seconds, which means Lambda waits up to 5 seconds to fill the batch before invoking.

Serverless Databases

DynamoDB: Serverless NoSQL

The Problem

Traditional databases require provisioning instances, managing capacity, handling replication, and patching. You want a database that scales like your functions.

How It Works

A DynamoDB table has a partition key, also called a hash key, which is required. For example, the user ID. It can optionally have a sort key, also called a range key, such as a timestamp.

DynamoDB offers two capacity modes. Provisioned capacity lets you set specific read capacity units and write capacity units. It's cheaper if your traffic is predictable. On-demand capacity charges you per request with automatic scaling and no capacity planning required.

When using DynamoDB with Lambda, you have several access patterns. For a point read to get a single item, you call get_item with the key. This is very fast, typically returning in single-digit milliseconds.

For a query using the partition key and a range on the sort key, you use the query method with a key condition expression. For example, to get all events for a user between two timestamps, you query where user ID equals a value and timestamp is between start time and end time. This is efficient because it only reads the relevant partition.

You should avoid scans whenever possible. A scan reads the entire table and then filters the results. For large tables, this is expensive and slow. For example, scanning for all premium users reads every single item in the table.

For batch writes, you can write up to 25 items atomically using a batch writer context manager.

For atomic counters, you can use update_item with an update expression like "SET login_count = login_count + :inc". This increments the counter without race conditions.

Trade-offs

For scaling, DynamoDB offers automatic, unlimited scaling, but you can experience throttling on hot partitions.

For cost, on-demand mode means you don't pay for unused capacity, but scans become expensive.

For performance, you get single-digit millisecond reads, but there are no complex joins or aggregations.

For operations, there's zero maintenance required, but schema design is critical to good performance.

When to use DynamoDB: Use it for key-value lookups, session storage, user profiles, and event logging.

When NOT to use DynamoDB: Avoid it for complex queries, ad-hoc analytics, and relational data with many joins.

Aurora Serverless: Serverless RDBMS

The Problem

You need SQL features like joins, transactions, and foreign keys, but you don't want to manage database instances.

How It Works

Aurora Serverless version 2 uses capacity units called ACUs. You set a minimum, such as 0.5 ACU, and a maximum, such as 128 ACUs. During low traffic, it might use just 0.5 ACU, which costs about 6 cents per hour. During high traffic, it might scale to 10 ACUs, costing about $1.20 per hour. The database auto-scales in seconds based on load. Storage also auto-scales, and you pay only for what you use.

When connecting from Lambda, you store database credentials in AWS Secrets Manager. In your Lambda function, you retrieve the credentials, then connect to the database via an RDS Proxy endpoint. The proxy is critical for connection pooling, which I'll explain in a moment. Once connected, you can execute normal SQL queries with joins, aggregations, and transactions.

For example, you might query to get user names and their order counts by joining the users and orders tables, filtering by creation date, and grouping by user ID.

Critical: RDS Proxy for Connection Pooling

Here's why RDS Proxy is essential: Without it, if Lambda spikes to 100 concurrent executions, you'd create 100 database connections. But databases typically have a maximum of only 150 connections, so you'd overload the database.

With RDS Proxy, even when Lambda spikes to 100 concurrent executions, the proxy maintains only 10 to 20 actual database connections and multiplexes requests across them. This keeps the database stable.

Trade-offs

For SQL support, you get full PostgreSQL or MySQL compatibility, but it's more expensive than DynamoDB.

For scaling, capacity auto-scales, but version 1 had cold start issues when the database was paused.

For cost, you pay for actual usage, but there's a minimum capacity charge.

For compatibility, it's a drop-in replacement for regular Aurora, but connection pooling is required to work well with Lambda.

Step Functions: Workflow Orchestration

The Problem

You need to coordinate multiple Lambda functions into a workflow with retries, error handling, and conditional logic. Embedding this coordination logic directly in your code creates spaghetti.

How It Works

Step Functions uses state machines to define workflows. Let me walk you through an order processing workflow.

The workflow starts with a "Validate Order" state. If validation succeeds, it proceeds to "Check Inventory". The workflow then has a choice state called "Inventory Available?" that examines the result. If inventory is available, it continues to "Charge Payment". If inventory is not available, it branches to "Out of Stock" and sends a notification before ending as a failure.

On the success path, after charging payment, the workflow proceeds to "Ship Order" and then "Send Receipt" before ending successfully.

Throughout this workflow, each state can have retry logic with exponential backoff and error handlers that catch specific error types and route to appropriate recovery states.

The state machine is defined using Amazon States Language, which is JSON. Let me describe the key parts.

For the Validate Order state, it's a Task type that invokes the validate-order Lambda function. It has a Next field pointing to Check Inventory. It has a Catch block that catches ValidationError and routes to the OrderFailed state. It also has a Retry configuration with an interval of 2 seconds, maximum of 3 attempts, and a backoff rate of 2.0, giving you exponential backoff.

The Check Inventory state is another Task that invokes check-inventory and routes to a choice state.

The Inventory Available choice state uses a condition on the inventory_available field. If it's true, the workflow goes to Charge Payment. Otherwise, it defaults to Out of Stock.

The Charge Payment state can catch PaymentFailed errors and route to a failure state, storing the error details in the result path.

Your individual Lambda functions stay simple. The validate order function just checks that the order has items and a valid total. If validation fails, it raises an error. Otherwise, it returns data that flows to the next state.

The check inventory function looks at the items and checks stock availability, returning whether inventory is available along with the order ID.

Step Functions Features

Step Functions provides built-in retry logic with exponential backoff and configurable max attempts. It offers error handling where you can catch specific errors and route to recovery states. It supports wait states to delay execution, such as waiting for human approval. It enables parallel execution to run multiple branches concurrently. It gives you visibility into exactly where a workflow is and maintains full execution history. And it supports long-running workflows of up to 1 year, compared to Lambda's 15-minute limit.

Trade-offs

For orchestration, Step Functions offers visual, declarative workflows, but the JSON configuration can get large.

For durability, workflows survive failures, but state transitions cost money.

For debugging, you get full execution history, but there's a learning curve for Amazon States Language.

For long-running processes, you can run workflows up to 1 year, but high-frequency workflows with millions per day become expensive.

When to use Step Functions: Use them for multi-step workflows, saga patterns, and human-in-the-loop processes.

When NOT to use Step Functions: Avoid them for simple sequential tasks where you could just call one Lambda from another, and for high-frequency micro-workflows running millions per day, which gets very expensive.

Vendor Lock-in and Portability

The Reality Check

Serverless is deeply integrated with cloud provider services. "Portable serverless" is largely a myth.

When you write AWS Lambda functions, you import boto3, which is the AWS SDK. You might use AWS-specific tools like Lambda Powertools. Your code directly couples to AWS services like S3, DynamoDB, and Secrets Manager.

Mitigation Strategies

Strategy 1: Abstraction Layer

You can create abstraction layers to isolate provider-specific code. You define a storage interface as an abstract base class with methods like get and put. Then you implement this interface for different providers.

For AWS, you create an S3Storage class that uses boto3 to interact with S3. For Google Cloud, you create a GCSStorage class that uses the Google Cloud storage client. In your Lambda handler, you inject the storage implementation through a factory based on environment configuration. Your business logic remains portable.

The trade-off is that abstraction adds complexity and performance overhead. It's only worth it if multi-cloud deployment is a real requirement, which it usually isn't.

Strategy 2: Framework Abstraction

You can use frameworks like Serverless Framework or AWS SAM that claim to support multiple clouds. You write a configuration file specifying the provider as AWS, Google, or Azure. You define functions and their HTTP triggers. Then you deploy using a command that specifies the provider.

The reality is that event formats differ between providers, features differ, and you'll still end up writing provider-specific code.

Cost Model: Pay-Per-Invocation vs Always-On

Serverless Pricing

Let me explain AWS Lambda pricing. Request charges are 20 cents per 1 million requests. Compute charges are approximately 0.0000166667 dollars per gigabyte-second.

For example, with 512 megabytes of memory and 100 milliseconds execution time, each invocation costs about 0.0000008333 dollars. With 1024 megabytes of memory and 100 milliseconds, each invocation costs about 0.0000016667 dollars.

Let's calculate the cost for 1 million requests per month using 512 megabytes of memory with an average duration of 200 milliseconds. The request cost is 1 million times 20 cents divided by 1 million, which equals 20 cents. The compute cost is 1 million times 0.2 seconds times 0.5 gigabytes times 0.0000166667 dollars, which equals $1.67. The total is $1.87 per month.

versus EC2 or Containers

Now let's compare to always-on infrastructure. A t3.medium instance has 2 vCPUs and 4 gigabytes of RAM. It costs 4.16 cents per hour, which is $30.37 per month. It can handle approximately 1,000 requests per second.

For the break-even calculation, serverless costs $1.87 for 1 million requests per month, which averages about 12 requests per second. A container costs $30.37 for any amount of traffic.

Serverless is cheaper if you have less than 10 million requests per month. Containers are cheaper if you have more than 10 million requests per month with consistent traffic.

Cost Optimizations

First, right-size your memory allocation. More memory means more CPU power, which often means faster execution and lower overall cost. For example, 512 megabytes with 500 milliseconds duration costs the same as 1024 megabytes with 250 milliseconds duration, but the latter is twice as fast.

Second, use batch processing. If you process 1 item per invocation, 1 million items means 1 million invocations costing 20 cents in request charges. If you process 100 items per invocation using SQS batching, 1 million items only requires 10,000 invocations, costing just 0.2 cents in request charges.

Third, keep functions warm for critical paths using Provisioned Concurrency. This costs about 0.000003472 dollars per gigabyte-second. Functions are always ready with no cold starts. Use this for less than 10% of traffic on critical endpoints.

Serverless Provider Comparison

Let me compare the major serverless providers across several dimensions.

For maximum execution time, AWS Lambda allows 15 minutes, Google Cloud Functions gen2 allows 9 minutes, Azure Functions premium tier allows unlimited time, and Cloudflare Workers allows 30 seconds on the free tier and 15 minutes on paid plans.

For maximum memory, AWS Lambda supports up to 10 gigabytes, Google Cloud Functions supports up to 32 gigabytes, Azure Functions supports up to 14 gigabytes, and Cloudflare Workers supports only 128 megabytes.

For cold start times with Python, AWS Lambda typically takes 200 to 500 milliseconds, Google Cloud Functions takes 300 to 800 milliseconds, Azure Functions takes 500 milliseconds to 2 seconds, and Cloudflare Workers takes less than 1 millisecond because it uses V8 isolates instead of containers.

For pricing per compute, AWS Lambda charges 0.0000166667 dollars per gigabyte-second, Google Cloud Functions charges 0.0000025 dollars per gigabyte-second, Azure Functions charges 0.000016 dollars per gigabyte-second, and Cloudflare Workers charges 50 cents per million requests.

For free tier, AWS Lambda offers 1 million requests per month, Google Cloud Functions offers 2 million requests per month, Azure Functions offers 1 million requests per month, and Cloudflare Workers offers 100,000 requests per day.

For edge locations, AWS offers Lambda@Edge with limited functionality, while Google and Azure don't offer edge deployment. Cloudflare Workers run globally at the edge as their primary model.

For supported runtimes, AWS, Google, and Azure support many languages including Python, Node, Java, Go, .NET, and Ruby. Cloudflare Workers only supports JavaScript and WebAssembly.

For VPC integration, AWS, Google, and Azure all support VPC networking. Cloudflare Workers run on the edge network without VPC integration.

Each provider is best for different scenarios. AWS Lambda is best if you're already on AWS, need AWS service integration, and want a mature ecosystem. Google Cloud Functions is best if you're on GCP and need BigQuery or Pub/Sub integration. Azure Functions is best if you're on Azure, have .NET workloads, or need Durable Functions for workflow orchestration. Cloudflare Workers are best for global edge compute, sub-10 millisecond latency requirements, and simple workloads.

Cloudflare Workers Unique Model

Cloudflare Workers run on the CDN edge at over 200 locations worldwide. They have sub-millisecond cold starts because they use V8 isolates instead of containers. You write JavaScript that executes close to the user anywhere in the world. For example, you can access the connecting IP address and the user's city from request headers, then return a personalized response.

Key Concepts Checklist

Let me summarize the key concepts you should understand. First, understand cold start causes and mitigation strategies, including package size optimization, provisioned concurrency, and runtime choice. Second, design event-driven architectures using S3, SQS, API Gateway, and EventBridge. Third, choose the appropriate serverless database, using DynamoDB for key-value patterns and Aurora for SQL requirements. Fourth, orchestrate complex workflows with Step Functions, implementing retries, error handling, and gaining visibility into execution. Fifth, calculate the cost break-even point between serverless and containers or VMs. Sixth, implement connection pooling for databases using RDS Proxy. Seventh, know when NOT to use serverless, such as for consistent high throughput, strict latency SLAs, or long-running tasks. Eighth, monitor and debug distributed serverless applications using X-Ray and CloudWatch.

Practical Insights

Cold Start Reality

Most workloads never notice cold starts. If 99% of your requests complete in under 100 milliseconds and 1% take 2 seconds due to cold starts, your 99th percentile latency is still 2 seconds, but that might be perfectly acceptable for your use case. Measure actual user impact before over-optimizing. Provisioned concurrency is expensive, so use it sparingly for truly latency-sensitive paths like checkout or payment processing.

The 15-Minute Wall

Lambda's 15-minute timeout is a hard limit. For workloads like video encoding, large data processing, or machine learning inference that run longer, use containers with ECS or Fargate, or use batch processing services like AWS Batch. Don't try to work around this limitation using Step Functions just to stay "serverless." Use the right tool for the job.

Database Connections Are Your Enemy

Here's a common problem: 100 concurrent Lambda executions create 100 database connections. Most databases max out at 100 to 500 connections, so you hit connection pool exhaustion and start getting errors.

There are three solutions. First, use RDS Proxy on AWS for connection multiplexing. Second, use HTTP-based databases like DynamoDB, FaunaDB, or PlanetScale that don't use traditional connection pooling. Third, use connection pooling libraries with time-to-live settings to limit connection lifetime.

Monitoring Is Harder, Not Easier

Serverless doesn't mean "no ops." You still need distributed tracing with tools like AWS X-Ray or Datadog APM. You need structured logging with correlation IDs to trace requests across functions. You need custom metrics for business logic, not just infrastructure metrics. And you need alerting on error rates, duration 99th percentiles, and throttling events.

The difference is this: Instead of monitoring 5 servers, you're now monitoring 50 functions across 20 event sources. The complexity shifts from infrastructure management to architectural complexity.

The Lock-In Calculation

Let's do some math on switching costs. The cost to switch providers equals the number of lines of provider-specific code multiplied by engineer hours required to rewrite it.

Suppose you have 50,000 lines of code, and 20% is AWS-specific code using services like S3, DynamoDB, SQS, and Step Functions. That's 10,000 lines to rewrite. If one engineer can port 500 lines per day, that's 20 days of work. At 8 hours per day, that's 160 engineer hours. At a fully-loaded cost of $200 per hour, the switching cost is $32,000.

Now compare this to your AWS bill. If you're paying $5,000 per month and potential savings on GCP would be $1,000 per month, your break-even point is 32 months.

Usually it's not worth switching. Lock-in is a business decision, not a technical one.

Serverless for Startups versus Enterprises

For startups, serverless is often the right default choice. You have zero servers to manage, scaling happens automatically, and you pay only for usage. You can ship features faster. Accept vendor lock-in as a reasonable trade-off for speed of development.

For enterprises, use serverless for specific use cases like event processing, APIs with spiky traffic, and glue code between systems. Keep your core business logic in containers or VMs for cost predictability and control at scale. A common pattern is using serverless for 20% of workloads and containers for 80%.

The Cold Start Distribution

Let me explain when cold starts actually happen. On the first invocation ever, you get 100% cold starts. During traffic spikes, you get roughly 10 to 30% cold starts as new containers spin up. After a deployment, you get 100% cold starts for the first requests. After an idle period, you get 100% cold starts because AWS reclaims containers after about 15 minutes of inactivity.

If you serve 1 million requests per day evenly distributed, that's about 12 requests per second on average. You'll probably have 2 to 5 warm containers, and your cold start rate will be around 1 to 5%.

If you serve 1 million requests per day with traffic spikes, you might hit 100 requests per second at peak. During those spikes, your cold start rate will be around 10 to 20%.

This concludes Chapter 29 on Serverless Architecture.
