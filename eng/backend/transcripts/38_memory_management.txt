Chapter 38: Memory Management

The Core Problem

Let's start with a scenario that illustrates why memory management matters. Imagine your production service is handling 10,000 queries per second with a 99th percentile latency of 200 milliseconds. Over the weekend, traffic spikes to 15,000 queries per second. Suddenly, latency jumps to 3 seconds. Your CPU usage spikes to 100 percent, but when you investigate, most of the time is being spent in garbage collection. Then the server starts swapping to disk, which creates a death spiral. Finally, the out-of-memory killer terminates your process, and you have a complete outage.

Without understanding memory management, you'll make poor decisions. You might blindly increase the heap size, which just leads to longer garbage collection pauses. Or you restart services, which is a temporary fix that doesn't address the root cause. Or you scale horizontally, which is expensive and doesn't actually solve the underlying problem.

But with proper memory management knowledge, you can identify memory leaks before they cause outages. You can tune garbage collection for your specific workload, choosing between throughput and latency. You can use memory pools to reduce allocation overhead. You can understand cache effects that make code 10 times faster. And you know when to use off-heap storage.

Memory is the foundation. Everything else builds on it. Now let's dive into the fundamentals.

Stack versus Heap Memory

The Fundamental Split

Every program divides memory into two regions with opposite characteristics: the stack and the heap.

The stack grows downward from high memory addresses. It stores local variables, function call information, and temporary data. It's fast because allocation is automatic. It's limited in size, typically between 1 and 8 megabytes per thread.

The heap grows upward from low memory addresses. It stores dynamically allocated objects, arrays, and long-lived data. It's flexible because you can allocate any size. It's unlimited, constrained only by available RAM and virtual memory. But it's slower and requires manual management in languages like C, or garbage collection in languages like Python and Java.

Stack: The Fast Path

Here's how the stack works. The stack pointer register, called ESP or RSP on x86 architectures, tracks the top of the stack. When you call a function, the system pushes the return address onto the stack and allocates space for the function's local variables. This is called a stack frame. When the function returns, it pops the frame and restores the stack pointer. All of these operations happen in CPU registers, which means they complete in nanoseconds.

Let me describe the memory layout. Imagine you have a function called foo that takes parameters x and y, and it calls another function called bar with parameter z. At the top, in high memory addresses, you have foo's local variables in foo's stack frame. Below that is the return address that tells the CPU where to go after bar finishes. Then you have saved registers from foo. Then you have bar's local variables in bar's stack frame. And the stack pointer points to the current top of bar's frame.

Here's a Python example to illustrate the concept, even though Python abstracts this away. When you define a function called calculate that takes x and y, all the local variables inside that function live on the stack. You might have a variable called result that equals x plus y, which takes 8 bytes for a 64-bit integer. You might have another variable called doubled that equals result times 2, which also takes 8 bytes. When you return doubled, the entire stack frame is destroyed on return. The stack frame is created and destroyed in nanoseconds. That's the beauty of stack allocation.

The stack has several key characteristics. First, it's extremely fast because you just adjust the stack pointer. Second, the size is limited, typically 1 to 8 megabytes per thread. Third, the lifetime is automatic—memory is automatically freed when the function returns. Fourth, there's no fragmentation because allocations are always contiguous. And fifth, each thread has its own stack, so there are no thread safety issues.

You should use the stack for local variables, function parameters, small arrays less than a few kilobytes, and short-lived data.

You should NOT use the stack for large objects, which will overflow the stack. Don't use it for data that needs to outlive a function call. Don't use it for dynamically sized data. And don't use it for data that needs to be shared between threads.

Heap: The Flexible Alternative

Now let's talk about the heap. The heap is managed by an allocator, which might be malloc and free in C, or a garbage collector in Python and Java. The allocator tracks free regions using data structures like free lists or bitmaps. When you request memory, the allocator finds a suitable free block. When you deallocate memory, it marks the block as free for reuse.

Let me describe a typical heap state. Imagine you have a heap that looks like this. There's a used block of 24 bytes, then a free block of 64 bytes, then a used block of 48 bytes, then a free block of 128 bytes, then a used block of 16 bytes. The allocator maintains a free list that points to the 64-byte block, which points to the 128-byte block, which points to NULL.

Here's a Python example. When you create a large list with 1 million zeros, that list lives on the heap. It's about 8 megabytes of memory. Unlike stack data, you can return heap-allocated data from a function. The stack frame gets destroyed, but the heap data survives. So if you create a function called process data that creates a large list and returns a dictionary containing that list, the heap allocation persists after the function returns.

The heap has different characteristics from the stack. It's slower because the allocator has to search free lists and manage metadata. The size is limited only by available RAM and virtual memory, so it's much more flexible. The lifetime is manual in languages like C, where you call malloc and free, or it's managed by garbage collection in languages like Python and Java. Fragmentation is a common problem with heaps. And heaps require synchronization for thread safety.

Let me summarize when to use stack versus heap. For a local counter, use the stack. For a 10-megabyte buffer, use the heap. If you need to return data from a function, use the heap. For per-thread caches, use the stack. For recursive call data, you might use the stack, but be careful of stack overflow.

Memory Allocation Strategies

The malloc Problem

When you call malloc, the allocator has to do several things. First, it needs to find a free block of sufficient size. Second, it might need to split a block if it's too large. Third, it returns a pointer to usable memory. And fourth, it needs to track allocation metadata like the size of the block.

The cost of malloc is typically 100 to 1000 CPU cycles, compared to 1 or 2 cycles for stack allocation. That's a significant overhead.

Strategy 1: Free List

One approach is to use a free list. The allocator maintains a linked list of free blocks. Each free block contains metadata about its size and a pointer to the next free block.

There are several strategies for choosing which block to allocate. First-fit finds the first block that's large enough. It's fast because it stops at the first match, but it has poor memory utilization because it leaves small gaps throughout the heap. Best-fit finds the smallest block that fits the request. It has better utilization, but it's slower because it must scan the entire list. And it creates tiny unusable fragments. Worst-fit finds the largest block. The idea is to leave large fragments for future large allocations, but this is rarely used because it has poor performance in practice.

Strategy 2: Segregated Free Lists

A better approach is to use segregated free lists. You maintain separate free lists for different size classes. For example, you might have a free list for 16-byte blocks, another for 32-byte blocks, another for 64-byte blocks, and so on up to 512 bytes. For anything larger, you have a separate list for large allocations.

The allocation algorithm is simple. When you request a certain size, you round up to the nearest size class. Then you look in the free list for that size class. If there's a free block, you just pop it off the list in constant time. If the list is empty, you allocate a new chunk from the operating system.

Segregated free lists have several advantages. Allocation is constant time, so it's very fast. Fragmentation is reduced because same-sized allocations are grouped together. And it's cache-friendly because similar objects are located near each other in memory.

This approach is used by tcmalloc, jemalloc, and glibc malloc.

Strategy 3: Memory Pools

Another strategy is memory pools, also called arena allocation. You pre-allocate a large chunk of memory and carve it into fixed-size blocks.

Here's a Python implementation to illustrate the concept. You create a class called MemoryPool that takes a block size and the number of blocks. In the initializer, you allocate a large byte array that's block size times the number of blocks. Then you initialize a free list. You iterate through all the blocks and append each block's offset to the free list.

To allocate, you check if the free list is empty. If it is, you raise a MemoryError. Otherwise, you pop an offset from the free list and return a memory view into that region of the pool.

To free, you simply append the offset back to the free list.

This is extremely fast because allocation and deallocation are just list operations.

You should use memory pools when you have many same-size allocations, such as fixed-size network packets. They're great when you have high allocation and deallocation frequency. And they provide deterministic allocation time, which is important for real-time systems.

A real-world example is a web server connection pool. Each connection object has a fixed size because it has a socket, a buffer, and a state field. You can create a memory pool for 10,000 connections. When a new connection arrives, you allocate from the pool instead of calling malloc, which eliminates the allocation overhead.

Garbage Collection Algorithms

Why GC Exists

In manual memory management languages like C and C plus plus, you call malloc to allocate memory and then you must call free to deallocate it. This creates several problems. If you forget to free, you have a memory leak. If you use memory after freeing it, you get a crash or security bug. If you free the same memory twice, you get memory corruption.

Garbage collection provides automatic safety at the cost of some performance overhead.

Reference Counting

The first garbage collection approach is reference counting. The idea is to track the number of references to each object. Every object has a reference count field. When you create an object, the reference count starts at 1. When you assign the object to another variable, you increment the reference count. When you delete a variable, you decrement the reference count. When the reference count reaches zero, you immediately deallocate the object.

Here's how it works visually. Imagine you have an object A with a reference count of 2. Two variables, x and y, both point to object A. When you delete variable x, the reference count decreases to 1. Object A is still alive. When you delete variable y, the reference count reaches zero, and the object is immediately deallocated.

Reference counting has several advantages. Deallocation is immediate—there are no garbage collection pauses. It's deterministic because you know exactly when objects are freed. And it's simple to understand.

But it has disadvantages too. There's overhead on every assignment because you have to update reference counts. And it cannot handle cycles. If object A has a reference to object B, and object B has a reference back to object A, both objects have a reference count of 1. But if no other variables reference them, they're unreachable from the program. They'll never be freed because their reference counts never reach zero.

Reference counting is used by Python as the primary garbage collection mechanism, as well as Swift and Objective-C.

Mark-and-Sweep

The second approach is mark-and-sweep. This algorithm has two phases. The mark phase traverses all reachable objects from roots, which are global variables and stack variables. The sweep phase frees all unmarked objects.

Here's how it works. Imagine you have several objects. Your roots point to object A and object D. Object A points to object B, which points to object C. Object D points to object E. And object F exists but is unreachable.

In the mark phase, you do a depth-first search from the roots. You mark A as true. Then you reach B from A, so you mark B as true. Then you reach C from B, so you mark C as true. You mark D as true because it's a root. Then you reach E from D, so you mark E as true. But you never reach F, so it remains marked as false.

In the sweep phase, you scan all objects. Any object where marked equals false gets freed. So object F is deallocated.

Mark-and-sweep has advantages. It handles cycles correctly because if a cycle is unreachable from roots, all objects in the cycle will be unmarked and freed. There's no per-assignment overhead like with reference counting.

But it has disadvantages too. It requires a stop-the-world pause where the entire program freezes during collection. And it can cause heap fragmentation because deallocating objects creates gaps in memory.

For a large heap with 10 million objects, the mark phase might take 50 milliseconds to traverse the object graph, and the sweep phase might take 20 milliseconds to scan all objects. That's a total pause of 70 milliseconds, which users will experience as a latency spike.

Generational GC

The third approach is generational garbage collection. This is based on a key insight: most objects die young. Empirical observation shows that 90 percent of objects become garbage within seconds of creation. Another 5 percent live a bit longer. And only 5 percent live for the entire program lifetime.

Generational GC divides the heap into generations. The young generation is where most allocations happen. It's small, maybe 10 megabytes, and it's collected frequently with fast garbage collection cycles that cause pauses of around 100 milliseconds. The old generation holds long-lived objects. It's much larger, maybe 1 gigabyte. It's collected infrequently, but when it is collected, it can cause pauses of 1000 milliseconds or more.

The algorithm works like this. Most allocations go into the young generation. When the young generation fills up, you run a minor garbage collection. During minor GC, you only collect the young generation. You mark from roots, but you stop when you reach objects in the old generation. This is much faster than collecting the entire heap. Objects that survive get their age incremented. If an object has survived three collections, you promote it to the old generation.

Eventually, the old generation fills up, and you run a major garbage collection that collects all generations. This is expensive, but it happens rarely.

Here's an example. Minor GC might run every 100 milliseconds and cause a 10-millisecond pause. Major GC might run every 60 seconds and cause a 500-millisecond pause. So 99.9 percent of garbage collection pauses are short, and only 0.1 percent are long.

Generational GC is used by the JVM with collectors like G1GC and ZGC, by .NET, and by Python for detecting reference cycles.

Copying GC

The fourth approach is copying garbage collection. You divide the heap into two semi-spaces. Only one space is active at any time. The other space is empty.

When garbage collection is triggered, you copy all live objects from the from-space to the to-space. As you copy them, you compact them so there are no gaps. Then you swap the roles of the two spaces. The to-space becomes the from-space, and the from-space becomes the to-space, which is now empty and ready for allocation.

Copying GC has several advantages. It provides automatic compaction, so there's no fragmentation. Allocation is extremely fast—you just bump a pointer. And the garbage collector only touches live objects; it completely ignores garbage.

The disadvantage is that you can only use 50 percent of the heap at any time because the other half must be kept empty for the next collection. Also, the pause time is proportional to the live set size, not the heap size.

Copying GC is typically used for young generation garbage collection where most objects are garbage and only a small amount of live data needs to be copied. It's also used when you need low fragmentation.

Memory Leaks

What Is a Memory Leak?

A memory leak is memory that's allocated but never freed, despite being unreachable by the program.

Here's a Python example. Imagine you have a global cache dictionary. You have a function called process request that takes a user ID. If the user ID isn't in the cache, you run an expensive computation and store the result in the cache. Then you return the cached value.

This seems fine, but after processing 1 million users, the cache has 1 million entries. Even if most users never return, their data stays in the cache forever. The memory is never freed. This is a memory leak.

Detecting Leaks

How do you detect memory leaks? Here are the symptoms. Memory usage grows over time. Memory never decreases, even under low load. Eventually the process crashes with an out-of-memory error. And restarting temporarily fixes the issue but doesn't address the root cause.

There are several detection tools. First, you can use memory profiling. In Python, you use the tracemalloc module. You start tracing, run your code, then take a snapshot. The snapshot shows you the top memory allocators by line number.

Second, you can use heap dump analysis. For Python, you use the guppy module. For the JVM, you use jmap to create a heap dump, then analyze it with Eclipse Memory Analyzer. You look for objects with high retained size.

Third, you can monitor growth rate. You use the psutil library to monitor your process's memory usage. You record a baseline, then every 60 seconds you check the current memory and calculate the growth percentage. If memory grows by more than 50 percent per hour, you alert about a possible memory leak.

Common Leak Patterns

Let me describe three common memory leak patterns. First, event listeners that aren't removed. Imagine you have a Dashboard class that subscribes to an event bus. In the close method, you forget to unsubscribe. The event bus still holds a reference to the Dashboard instance, so it never gets garbage collected. The fix is to call unsubscribe in the close method.

Second, circular references with destructors. In Python, if you define a del method, which is a destructor, the garbage collector won't automatically break cycles. Imagine you have a Parent class that creates a Child, and the Child holds a reference back to the Parent. This creates a cycle. If either class defines del, the cycle won't be broken, and both objects leak. The solution is to avoid del or use weak references.

Third, unbounded caches. A bad pattern is to use a plain dictionary as a cache with no size limit. A good pattern is to use an LRU cache with a maximum size, like Python's functools.lru_cache decorator with maxsize equals 10,000.

Memory Fragmentation

The Problem

Memory fragmentation happens after many allocations and deallocations. Imagine your heap looks like this. You have a used block of 32 bytes, then a free block of 8 bytes, then a used block of 16 bytes, then a free block of 12 bytes, then a used block of 24 bytes, then a free block of 4 bytes, then a used block of 40 bytes.

The total free space is 24 bytes. But if you request 16 bytes, the allocation fails because the free space is fragmented into small chunks. No single chunk is large enough.

External versus Internal Fragmentation

There are two types of fragmentation. External fragmentation is when free space is divided into small, unusable gaps. It's caused by varying allocation sizes.

Internal fragmentation is waste within allocated blocks. The allocator rounds up your request. For example, if you request 25 bytes but the allocator uses 32-byte size classes, it allocates 32 bytes. You use 25 bytes, and 7 bytes are wasted. That's internal fragmentation.

Solutions

There are several solutions to fragmentation. First, use a compacting garbage collector that moves objects to eliminate gaps. Copying GC does this automatically. Second, use memory pools with fixed-size allocations to avoid fragmentation. Third, use a buddy allocator that splits blocks in powers of 2, which keeps free chunks at clean power-of-2 sizes.

Memory-Mapped Files

The Concept

Memory-mapped files let you map a file directly into your process's virtual memory. The file appears as a byte array in memory. The operating system handles caching and paging automatically.

Here's how it looks. Your process has a code segment, a heap, and a memory-mapped file region. That region appears as a byte array with indices from 0 to 1 million. It's backed by a disk file, like slash data slash log.txt.

Here's a Python example. You open a file, then use the mmap module to memory-map it. You can access it like an array. To read byte 1000, you just index into the memory map. To write byte 1000, you assign a value. There's no need for explicit read or write calls. The operating system handles everything.

You should use memory-mapped files for large files greater than 100 megabytes, for random access patterns, for shared memory between processes, and in database implementations.

The benefits are that the OS handles caching automatically, multiple processes can share the same physical pages, and the file is loaded lazily—only the pages you actually access are loaded into memory.

Cache Locality and Memory Access Patterns

Why Locality Matters

Modern CPUs have a memory hierarchy. At the top, CPU registers take 1 cycle, which is 0.3 nanoseconds. That's the fastest. L1 cache takes 4 cycles, or 1 nanosecond, and is 32 kilobytes. L2 cache takes 12 cycles, or 3 nanoseconds, and is 256 kilobytes. L3 cache takes 40 cycles, or 10 nanoseconds, and is 8 megabytes. RAM takes 200 cycles, or 60 nanoseconds. That's much slower. An SSD takes 50,000 cycles, or 15 microseconds, which is 250 times slower than RAM. And a hard drive takes 10 million cycles, or 3 milliseconds, which is 50,000 times slower than RAM.

The cache line is 64 bytes, which is the unit of data loaded at once.

Sequential versus Random Access

Sequential access is cache-friendly. Imagine you have an array of 10 million integers. You iterate through it sequentially and sum the values. This might take 150 milliseconds. The next element you need is likely already in the cache because the cache loads 64-byte chunks.

Random access is cache-hostile. If you shuffle the indices and access the array in random order, the same operation might take 1.5 seconds. That's 10 times slower. Every access might be a cache miss, requiring a fetch from RAM.

Struct of Arrays versus Array of Structs

There are two ways to organize data. Array of structs means each element is a complete structure. For example, each particle has x, y, vx, and vy fields. When you update positions, you access x and vx for each particle, but those fields are scattered in memory. The cache loads all four fields, but you only use two, which wastes bandwidth.

Struct of arrays means you have separate arrays for each field. All x values are in one array, all vx values are in another array. Now when you update positions, you iterate through the x array and the vx array sequentially. This is much more cache-friendly.

In practice, array of structs might take 150 milliseconds, while struct of arrays takes 50 milliseconds. That's a 3x speedup from just changing the data layout.

False Sharing

The Problem

False sharing occurs when two threads access different variables that happen to be on the same cache line. Imagine thread 1 writes to counter A, and thread 2 writes to counter B. If both counters are on the same 64-byte cache line, here's what happens. Core 1 modifies counter A, which invalidates the cache line on core 2. Core 2 must reload the cache line, even though it only needs counter B. Then core 2 modifies counter B, which invalidates the cache line on core 1. This ping-pong effect causes constant cache invalidation.

A bad pattern is to have two counters as adjacent fields in a class. They're in the same cache line, which causes false sharing. The program might take 2.5 seconds due to cache thrashing.

Solution: Padding

The solution is to add padding between the counters so they're on different cache lines. You add 64 bytes of padding between counter A and counter B. Now they're on different cache lines, and there's no false sharing. The program now takes 0.3 seconds. That's 8 times faster.

Python Memory Management

Reference Counting

Every Python object has a reference count. You can see it with sys.getrefcount. When you create a list, the reference count is 2: one for the variable and one for the argument to getrefcount. When you assign the list to another variable, the reference count becomes 3. When you delete one variable, it goes back to 2.

In the CPython implementation, every object has a reference count field and a type field. Every assignment increments the reference count. Every delete decrements it. When the reference count reaches zero, the object is immediately deallocated.

Cycle Detection

Reference counting can't handle cycles. If you create two lists, A and B, and append B to A and append A to B, you have a cycle. Even if you delete both variables, the cycle still exists in memory. Each object has a reference count of 1 from the other object.

Python has a cyclic garbage collector to handle this. You can force garbage collection with gc.collect. If you know your code doesn't create cycles, you can disable it with gc.disable for better performance.

Memory Pools

Python uses a custom allocator called pymalloc for small objects less than 512 bytes. It maintains segregated free lists for different size classes. Large objects use the system's malloc and free.

You can optimize by reusing objects with an object pool. You maintain a pool of objects. When you need one, you pop from the pool instead of allocating. When you're done, you reset its state and return it to the pool for reuse.

JVM Memory Model and GC Tuning

JVM Memory Regions

The JVM heap has several regions. The young generation is where most allocations happen. It's divided into Eden, which is 80 percent of the young generation, and two survivor spaces, S0 and S1, each 10 percent. Most objects die in the young generation.

Survivors are promoted to the old generation, which holds long-lived objects. Objects are tenured after surviving N young GC cycles.

There's also metaspace for class metadata, which used to be called PermGen. It's not part of the heap; it's in native memory.

GC Algorithms

The JVM offers several garbage collectors. Serial GC is single-threaded and stop-the-world. It's good for single-core systems with heaps under 100 megabytes. Parallel GC is multi-threaded and optimizes throughput. It's good for batch processing. G1 GC divides the heap into regions and provides predictable pause times. It's good for large heaps over 4 gigabytes and low-latency requirements. ZGC provides sub-10-millisecond pauses and is mostly concurrent. It's good for ultra-low latency with p99 under 10 milliseconds.

GC Tuning

Common JVM flags include Xms for initial heap size, Xmx for maximum heap size, XX:+UseG1GC to use the G1 collector, XX:MaxGCPauseMillis to set a target pause time, and various flags for logging GC activity.

You analyze GC logs to see pause times and frequencies. For example, a log entry might show that a young generation GC pause took 23 milliseconds. Eden went from 512 megabytes to zero. Survivors grew. And the overall heap went from 2 gigabytes to 1.6 gigabytes.

Your tuning strategy is to monitor GC frequency and pause times first. If you see frequent minor GCs, increase the young generation size. If you see long major GC pauses, increase the overall heap size or switch to a different garbage collector. If throughput is low, reduce GC overhead by using a larger heap.

Here's an example scenario. You have 500-millisecond GC pauses every 2 seconds. You analyze the logs and find that the old generation is filling up quickly because objects are being promoted too early. Your solution is to increase the young generation size by setting NewRatio to 1, which makes young and old equal in size instead of the default ratio of 1 to 2. You also increase MaxTenuringThreshold to 15 so objects go through more cycles before promotion. After tuning, you get 50-millisecond pauses every 10 seconds. That's a huge improvement.

Key Concepts Checklist

Here are the key concepts you should be able to explain. First, explain stack versus heap trade-offs, focusing on speed versus flexibility. Second, describe at least two memory allocation strategies, like free lists and memory pools. Third, compare reference counting versus mark-sweep garbage collection. Fourth, discuss generational GC and why it works based on the observation that most objects die young. Fifth, identify common memory leak patterns like unbounded caches and unreleased event listeners. Sixth, explain memory fragmentation and solutions like compacting GC. Seventh, describe cache locality's impact on performance and how sequential access is much faster than random access. And eighth, know how to tune garbage collection for your specific language and runtime.

Practical Insights

Memory is the bottleneck, not CPU. Modern systems spend more than 50 percent of their time waiting for memory. A single cache miss costs 200 CPU cycles. You should optimize for cache locality first, and only then optimize algorithms.

Monitor allocator performance. Tools like jemalloc can be 2 to 3 times faster than the default malloc for certain workloads. Profile your application before choosing an allocator. Consider tcmalloc for multi-threaded servers and mimalloc for general use.

GC tuning is workload-specific. Don't copy settings from blog posts. Run your actual workload, measure your actual pauses, and tune based on your p99 latency requirements. Start with default settings, change one flag at a time, and measure the impact.

Object pools pay off at high scale. Below 10,000 allocations per second, pools add complexity without benefit. Above 100,000 allocations per second, pools can reduce GC pressure by 10x and eliminate tail latency from allocation storms.

False sharing kills multi-core performance. If your multi-threaded code doesn't scale linearly with the number of cores, check for false sharing. Use cache line padding for frequently updated counters. Tools like perf c2c on Linux or Intel VTune can help you identify false sharing.

Python's GIL makes memory management easier but limits parallelism. Reference counting prevents data races, but it means CPU-bound Python code is effectively single-threaded. Use multiprocessing for CPU parallelism and threading for I/O-bound work. Or migrate performance-critical paths to C extensions.

This concludes Chapter 38 on Memory Management.
