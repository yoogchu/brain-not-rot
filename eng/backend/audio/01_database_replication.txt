Chapter 1: Database Replication

The Core Problem

Let's start by understanding why database replication matters. Imagine your production database is serving 50,000 queries per second. At 3 AM, the primary server's solid-state drive fails. If you don't have replication in place, here's what happens. You need to restore from last night's backup, which means you've lost 8 hours of transactions. The recovery process takes 2 or more hours while your customers can't complete any transactions. The revenue loss could be catastrophic for your business.

But now consider the same scenario with properly configured replication. A replica gets promoted to primary in just a few seconds. With synchronous replication, you have zero data loss. Your customers experience a brief blip rather than an extended outage. That's a massive difference.

However, replication isn't just about disaster recovery. It enables several other critical capabilities. First, read scaling. You can route read requests to replicas while all writes go to the primary. Second, geographic distribution. You can serve users from replicas that are physically close to them, reducing latency. Third, analytics isolation. You can run heavy analytical queries on dedicated replicas without impacting your production workload. And fourth, rolling upgrades. You can upgrade your database replicas one by one without downtime.

Now let's talk about the different types of replication, starting with the most common pattern.

Single-Leader Replication

The Basic Idea

In single-leader replication, one node, which we call the leader, primary, or master, accepts all writes. Changes then propagate to followers via Write-Ahead Log shipping or logical replication.

Think of this like a newspaper publishing operation. There's one editor-in-chief who is the leader, and that person approves all changes. Then those changes get distributed to all the newsstands, which are the followers.

Here's how the architecture looks. The leader node handles all writes. It then streams the Write-Ahead Log to follower number 1 and follower number 2. These followers are used for reads only.

How WAL Shipping Works

The Write-Ahead Log, or WAL, is the database's journal. Before any change happens to the actual data pages, it's first written to the WAL. This ensures durability. If the system crashes, it can replay the WAL to recover all committed transactions.

Here's the step-by-step process. First, a client sends a SQL update statement like "UPDATE users SET balance equals 100 WHERE id equals 123". Second, the leader writes to the WAL with an entry saying "Set balance equals 100 for user 123". Third, the leader applies the change to its data pages. Fourth, the leader streams the WAL entry to all followers. And fifth, the followers replay that WAL entry against their own data pages.

There are two types of replication: physical and logical. Physical replication, also called WAL shipping, replicates the raw byte-level changes. The advantage is that it creates an exact copy and is simpler to implement. The disadvantage is that all nodes must run the same database version. Logical replication, on the other hand, replicates SQL-level changes. The advantage is that it works across different versions and you can selectively replicate certain tables. The disadvantage is that it's more complex and slower.

Synchronous versus Asynchronous Replication

This is one of the most important trade-offs in distributed systems. Let me explain both approaches.

Synchronous Replication

In synchronous replication, here's the timeline of events. At time zero, the client sends a write to the leader. At time 1, the leader writes to its local disk. At time 2, the leader sends the data to a follower. At time 3, the follower writes the data to its disk. At time 4, the follower sends an acknowledgment back to the leader. And finally at time 5, the leader sends an acknowledgment to the client.

The key point is that the client doesn't get a success response until the follower has confirmed it received and persisted the data.

What does this guarantee? If the leader crashes after sending the acknowledgment to the client, the data is safe on the follower. The follower can be promoted to primary with zero data loss.

But what does this cost you? First, latency. Every write has to wait for a network round-trip to the follower. Second, availability. If the follower is slow or completely down, writes will block and your system becomes unavailable.

Asynchronous Replication

Now let's look at asynchronous replication. The timeline is different. At time zero, the client sends a write to the leader. At time 1, the leader writes to its local disk. At time 2, the leader immediately sends an acknowledgment to the client. And that's it from the client's perspective. Then at time 3, the leader eventually sends the data to followers. And at time 4, the followers write the data to disk.

The client gets a success response immediately after the leader persists the data, without waiting for followers.

What does this guarantee? You get low-latency writes. And the leader can continue operating even if followers are down, giving you higher availability.

But what are the risks? If the leader crashes between time 2 and time 4, that data is completely lost. It was acknowledged to the client but never made it to any followers. Also, followers may lag behind by minutes during periods of high load.

Semi-Synchronous: The Middle Ground

There's a hybrid approach called semi-synchronous replication. In this mode, you wait for ONE follower to acknowledge, but replication to the rest is asynchronous.

So the architecture looks like this. The leader has synchronous replication to follower 1, which must acknowledge before the client gets a success response. But replication to followers 2 and 3 is asynchronous and eventually consistent.

MySQL's semi-sync replication works this way. PostgreSQL's synchronous standby names configuration parameter can be set up similarly.

Let me summarize the trade-offs. For durability, synchronous is strong, asynchronous is weak, and semi-sync is medium. For write latency, synchronous is high because you add a network round-trip time, asynchronous is low, and semi-sync is medium. For availability, synchronous is lower, asynchronous is higher, and semi-sync is medium. And for use cases, synchronous is best for financial systems where you cannot lose data, asynchronous is best for high-throughput systems where some data loss is acceptable, and semi-sync is good for general-purpose applications.

Replication Lag: The Sneaky Problem

Asynchronous replication creates a time window where follower data is stale. This can cause real problems. Let me give you an example.

Here's the timeline. At time zero, a user updates their profile picture. The write goes to the leader. At time 1, the leader returns success to the user. At time 2, the user refreshes the page, and their request happens to get routed to a follower. At time 3, the follower hasn't received the update yet, so the user sees their old picture. This creates a "what the heck" moment for the user.

This isn't a bug. It's the inherent nature of asynchronous replication. But it creates terrible user experience.

Read-After-Write Consistency Patterns

To solve this problem, engineers have developed several patterns. Let me walk through the three most common ones.

Pattern 1 is called "read from leader after writes". Here's how it works in code. When you fetch a user profile, you check if the user just updated their data. If they did, you route the read request to the leader to get fresh data. Otherwise, you route to a replica for scalability.

The implementation is simple, but it requires tracking whether a user "just updated" something. This is often done with a session flag that expires after a few seconds, or by checking if the user's write timestamp is within the known lag window.

Pattern 2 is called "monotonic reads". The idea is to pin each user to the same replica for their entire session. You can do this by hashing the user ID to select which replica they should use.

With this approach, a user might see stale data, but they'll never go backwards in time. They'll never see a newer version and then an older version on the next request.

Pattern 3 is called "versioned reads". This is more sophisticated. You include a write timestamp or version number in every response. For example, your response includes the user profile data plus a version field showing "2024-01-15 at 10:30 AM".

Then the client sends that version in the next request. The replica waits until it has caught up to at least that version before responding. This is more complex to implement but provides strong guarantees.

Failover: The Hard Problem

Moving on to failover. When the leader dies, a follower must become the new leader. This sounds simple but is surprisingly tricky. Let me explain why.

Detection: Is the Leader Actually Dead?

The first challenge is detection. How do you know if the leader is actually dead?

The problem is that a network partition looks identical to an actual crash from the outside. Let me illustrate with two scenarios.

Scenario 1: The leader actually crashed. The leader stops sending heartbeats. A follower observes "no heartbeat" and concludes "the leader is dead". The correct action is to elect a new leader.

Scenario 2: There's a network partition. The leader is fine and still serving writes, but the network between leader and follower is broken. The follower observes "no heartbeat" and thinks "the leader is dead". But the reality is the leader is fine. If the follower elects itself as the new leader, you now have a split-brain scenario with two leaders.

The typical approach is using heartbeat timeouts. The leader sends a heartbeat every 5 seconds. If followers don't receive a heartbeat for 30 seconds, they assume the leader is dead.

But this has trade-offs. If the timeout is too short, you get false positives during garbage collection pauses or network blips. If the timeout is too long, you have extended downtime during real failures.

Election: Which Follower Becomes Leader?

Once you've decided the leader is dead, you need to choose which follower becomes the new leader. There are three main approaches.

Option 1 is to choose the most up-to-date replica. For example, follower A has data up to WAL position 1000, follower B has data up to WAL position 1050, and follower C has data up to position 1020. You choose follower B because it has the most recent data. This minimizes data loss and is the most common approach.

Option 2 is to use predetermined priority. You configure a priority list like "always try follower A first, then B, then C". This is predictable but might elect a less up-to-date replica.

Option 3 is to use consensus among replicas. You use an algorithm like Raft or Paxos for replicas to vote on the new leader. This is more complex but handles edge cases better.

Client Reconfiguration

Once you have a new leader, all clients must discover it. There are several approaches.

The first is a DNS update. Before failover, "primary.db.example.com" points to IP address 10.0.0.1, which is the old leader. After failover, you update the DNS to point to 10.0.0.2, which is the new leader. The problem is DNS caching. Clients may use the stale address for minutes due to DNS time-to-live settings.

The second approach is service discovery using tools like Consul or ZooKeeper. Clients watch a key like "/services/database/primary". When the value changes from "10.0.0.1" to "10.0.0.2", clients immediately reconnect.

The third approach is a proxy layer. Clients connect to HAProxy instead of directly to the database. HAProxy continuously health-checks all database nodes and automatically routes traffic to the current leader. This is transparent to clients.

Split-Brain: The Nightmare Scenario

Now let me explain the split-brain problem, which is one of the worst scenarios in distributed systems.

Here's what happens. You start with a normal state. Leader A has followers B and C replicating from it. Then a network partition occurs. Leader A becomes isolated on one side of the partition. Followers B and C are on the other side.

From A's perspective, it thinks it's still the leader and continues accepting writes. But B and C can't reach A, so they elect B as the new leader. Now B also starts accepting writes. You have two leaders, both accepting writes, and the data diverges. When the network heals, you have conflicting data that's very difficult to reconcile.

There are three main solutions to prevent split-brain.

Solution 1 is fencing tokens. The old leader has a token number like 42. When the new leader is elected, it gets token number 43. The storage system is configured to reject any writes with a token less than 43. So even if the old leader's writes eventually reach the storage layer, they're rejected. This prevents data corruption.

Solution 2 is called STONITH, which stands for "Shoot The Other Node In The Head". Before promoting a new leader, you send a power-off command to the old leader's management interface, like its IPMI or lights-out management. You wait for confirmation that the old leader is completely powered off. Only then do you promote the new leader. This sounds violent, but it's the only way to be absolutely certain the old leader can't accept writes.

Solution 3 is quorum-based leadership. To be the leader, you must maintain connections to a majority of nodes. In a 5-node cluster, you need at least 3 nodes agreeing you're the leader. If a partition splits the cluster into 2 nodes on one side and 3 on the other, the side with 2 nodes can't get a majority, so it steps down. The side with 3 nodes has a majority and continues as leader. This mathematically prevents split-brain.

Multi-Leader Replication

When Single-Leader Isn't Enough

Now let's move on to multi-leader replication. There are scenarios where single-leader replication isn't sufficient.

The first scenario is multi-datacenter deployment. Imagine users in New York City connecting to a datacenter in NYC. If that datacenter has to replicate every write across the Atlantic Ocean to a datacenter in London, you're adding 150 milliseconds of latency. Every write from London users has a 300-millisecond-plus round-trip. That's unacceptable for many applications.

The second scenario is offline-capable clients. Mobile apps need to work offline. Each device is essentially a "leader" for its local data that needs to sync when it comes back online.

The third scenario is extremely high write throughput. At extreme scale, a single leader can become a CPU bottleneck.

Architecture

In multi-leader replication, you have multiple datacenters, each with its own leader. For example, datacenter US-East has Leader A handling US writes. Datacenter EU-West has Leader B handling EU writes. These leaders replicate to each other asynchronously.

From the user's perspective, US users connect to US-East and get low latency. EU users connect to EU-West and also get low latency. This is much better than forcing all writes through a single global leader.

The Fundamental Challenge: Conflicts

But multi-leader replication introduces a fundamental challenge: conflicts. When two leaders modify the same data simultaneously, you have a problem.

Here's an example timeline based on wall clock time. At time zero, a user in New York City sets the title field to "Hello" in the US-East datacenter. At the exact same time zero, a user in London sets the title field to "Bonjour" in the EU-West datacenter. At time 1, the changes replicate asynchronously to the other datacenter. At time 2, both datacenters have conflicting versions of the same record.

Which value should win? This is not a trivial question.

Conflict Resolution Strategies

Let me walk through the three main strategies for resolving conflicts.

Strategy 1: Last-Write-Wins (LWW)

The first strategy is called Last-Write-Wins, abbreviated LWW. You use timestamps to pick the winner.

In our example, the US write has title "Hello" with timestamp 1000000001. The EU write has title "Bonjour" with timestamp 1000000002. The EU write wins because its timestamp is higher.

But here's the problem: clocks aren't perfectly synchronized across datacenters. Let's say the actual event order was that the US wrote first, but the EU datacenter's clock is 5 milliseconds ahead. The EU timestamp will be higher, so the EU write "wins". But this means the US user's write silently disappears.

NTP, which is the Network Time Protocol, provides approximately 100 milliseconds of accuracy over the internet, and 1 to 10 milliseconds on a local network. That's a large conflict window where writes can be incorrectly ordered.

Last-Write-Wins is acceptable in certain scenarios. First, idempotent operations where you're just setting a value like "SET x equals 5". Second, immutable event logs that are append-only. Third, cache updates where staleness is okay. Cassandra and DynamoDB use Last-Write-Wins as their default conflict resolution strategy.

Strategy 2: CRDTs (Conflict-free Replicated Data Types)

The second strategy uses special data structures called CRDTs, which stands for Conflict-free Replicated Data Types. These are mathematically guaranteed to converge regardless of the order in which operations are applied.

Let me explain with an example called a G-Counter, which is a grow-only counter. Here's how it works.

Each node maintains its own count in a dictionary. The node ID is the key and the count is the value. When a node wants to increment, it only increments its own counter in the dictionary. To get the total value, you sum up all the individual counts. To merge state from another node, you take the maximum of each node's count.

Here's a concrete example. Node A has a dictionary showing "Node A: 5, Node B: 0" for a total of 5. Node B has a dictionary showing "Node A: 3, Node B: 7" for a total of 10. When they merge, the result is "Node A: 5, Node B: 7" for a total of 12. This is correct!

Why does this work? Because each node only increments its own counter. When merging, you take the maximum value for each node's counter. There's no possibility of conflicts because nodes don't modify each other's counters.

There are several common types of CRDTs. A G-Counter is a grow-only counter, useful for like counts. A PN-Counter supports both increment and decrement, useful for account balances. A G-Set is an add-only set, useful for tag collections. An OR-Set supports both add and remove operations, useful for shopping carts. An LWW-Register is a single value with a timestamp, useful for profile fields.

CRDTs are used by Riak, Redis in CRDT mode, Automerge, and Yjs.

Strategy 3: Custom Application Resolution

The third strategy is to store all conflicting versions and let the application decide how to resolve them.

For example, your database might store a document with ID "doc123" that has a conflicts array containing version IDs like "2-abc" and "2-def". Then you store the actual data for each version separately.

The application can then do several things. It can merge the versions intelligently by combining fields from both. It can present the conflict to the user with a prompt like "Which version do you want to keep?". Or it can apply domain-specific business rules to automatically choose the right version.

This approach is used by CouchDB and PouchDB.

Leaderless Replication

Dynamo-Style Architecture

Now let's talk about leaderless replication. In this model, there's no leader at all. Any node can accept reads or writes.

Here's how it works. When a client sends a write request, it goes to multiple nodes in parallel. For example, it might go to Node A, Node B, and Node C simultaneously. All three nodes write the data. The client receives an acknowledgment when a certain number of nodes, which we call W, have responded.

This architecture is used by Cassandra, DynamoDB, Riak, and Voldemort.

Quorum Mathematics

Leaderless replication uses quorum mathematics to provide consistency guarantees. Here are the key parameters.

N is the total number of replicas. W is the write quorum, meaning the number of nodes that must acknowledge a write before it's considered successful. R is the read quorum, meaning the number of nodes you read from.

The guarantee is this: if W plus R is greater than N, then at least one node in your read quorum will have the latest write.

Let me give you an example. Suppose N equals 3, W equals 2, and R equals 2.

When you write, you write to Node A and Node B. That satisfies W equals 2. When you read, you read from Node B and Node C. That satisfies R equals 2. Notice that Node B appears in both sets. This overlap guarantees that you'll read the latest value because Node B has it.

Visually, you have three nodes: A, B, and C. The write went to A and B. Node C is stale. When you read from B and C, node B in the middle has the latest data, so your read succeeds with fresh data.

Tuning Quorums

You can tune the quorum parameters based on your workload. Let me describe the common configurations.

If you set W equals N and R equals 1, you're writing to all nodes but reading from any single node. This is good for read-heavy workloads where you want high availability for reads.

If you set W equals 1 and R equals N, you're writing to any single node but reading from all nodes. This is good for write-heavy workloads where you want high availability for writes.

If you set W and R to both be N divided by 2 plus 1, you get a balanced configuration. This is good for general-purpose applications.

If you set W equals 1 and R equals 1, you have no consistency guarantee. This provides maximum availability but only best-effort consistency.

Sloppy Quorums and Hinted Handoff

Here's an interesting question: what happens if one of the required nodes is unreachable during a write?

With a strict quorum, the write fails if you can't reach W designated nodes.

With a sloppy quorum, you write to W nodes even if they're not the designated replicas.

Let me give you an example. Suppose the designated replicas for a piece of data are Nodes A, B, and C. During a write, Node A is down.

With strict quorum, the write fails because you can't reach all the designated nodes.

With sloppy quorum, you write to Nodes B, C, and D instead, even though D isn't a designated replica. Node D stores the data along with a hint saying "this data belongs to Node A".

Later, when Node A recovers, Node D checks "Is Node A back online?". If yes, Node D transfers the data to Node A. This process is called hinted handoff. After the transfer, Node D deletes its copy.

Let me summarize the trade-off. For availability, strict quorum is lower and sloppy quorum is higher. For consistency, strict quorum is stronger because the W plus R greater than N property holds, while sloppy quorum is weaker. For use cases, strict quorum is appropriate for banking and inventory systems, while sloppy quorum is appropriate for shopping carts and other eventually-consistent applications.

The key insight is that sloppy quorum breaks the W plus R greater than N guarantee. Why? Because you might write to nodes B, C, and D, then later read from nodes A, B, and C. Only Node B overlaps, which isn't enough to guarantee consistency if there were concurrent writes.

Anti-Entropy and Read Repair

How do stale replicas catch up in a leaderless system? There are two main mechanisms.

The first is read repair. Here's how it works in code. When you read a key, you fetch it from R replicas. You find the response with the highest version number. Then you check if any replicas returned stale data. For each stale replica, you update it with the latest value.

This works well for frequently read data, but data that's never read stays stale forever.

The second mechanism is an anti-entropy process using Merkle trees. This runs in the background continuously.

Here's how it works. Each node builds a Merkle tree over its data. A Merkle tree is a hash tree where each leaf is a hash of a data range, and each parent is a hash of its children.

To compare with another node, you start by comparing root hashes. If they're different, you drill down to find which subtree differs. You keep drilling down until you find the specific data ranges that differ. Then you sync only those ranges.

This is very efficient because you can identify differences by comparing a small number of hashes instead of comparing all the data.

For example, Node A and Node B compare their root hashes and find they're different. They then compare the next level and find that one subtree differs. They drill down into that subtree and identify that data range 2 is different. They only need to sync data from range 2, not the entire database.

Replication Topology Summary

Let me summarize the different replication topologies we've discussed.

Single-leader with synchronous replication provides strong consistency, but lower availability and high write latency. The complexity is low.

Single-leader with asynchronous replication provides eventual consistency, higher availability, and low write latency. The complexity is still low.

Multi-leader replication provides eventual consistency with the possibility of conflicts, high availability, and low write latency because writes are local. However, the complexity is high.

Leaderless replication provides tunable consistency, the highest availability, tunable write latency, and medium complexity.

Key Concepts Checklist

When discussing replication in an interview, make sure you cover these key areas.

First, clarify the durability requirements. Can the system afford to lose any data?

Second, discuss the failover strategy and timing. How quickly does failover need to happen?

Third, address the consequences of replication lag. What happens if replicas are behind?

Fourth, consider geographic distribution. Are users spread across different regions?

Fifth, mention specific technologies like PostgreSQL, Cassandra, or MySQL and their replication features.

Sixth, quantify the latency impact. For example, synchronous replication adds at least one network round-trip time to every write.

Practical Insights

Let me share some practical insights from real-world production systems.

AWS RDS Multi-AZ failover takes 60 to 120 seconds. If you need faster failover, consider Amazon Aurora, which takes approximately 30 seconds, or Aurora Global Database, which has about a 1-minute recovery point objective and 1-minute recovery time objective.

PostgreSQL streaming replication lag is typically less than 1 second under normal load. However, it can spike to minutes during heavy write loads or network issues. You should monitor replication lag continuously.

Cassandra's default consistency level is ONE for both reads and writes. This means eventual consistency by default. If you need stronger guarantees, use the QUORUM consistency level for both reads and writes.

And that concludes Chapter 1 on Database Replication.