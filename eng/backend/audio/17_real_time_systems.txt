Chapter 17: Real-time Systems

Why Real-time?

Let's start by understanding why real-time communication matters in modern systems. Without real-time communication, applications face serious limitations.

Imagine a stock trader using HTTP polling, where they poll every one second for price updates. The problem is that stock prices can move in just 100 milliseconds, so the trader is seeing stale data. The result is lost trades and angry users.

Or consider a chat application using polling with 10,000 users. If each user polls every second for messages, that's 10,000 requests per second even when the system is idle. The result is server overload and battery drain on mobile devices.

Real-time communication enables several critical features. First, instant updates for applications like chat, notifications, and live feeds. Second, collaborative features like Google Docs or Figma where multiple users work together. Third, live dashboards for monitoring and analytics. And fourth, gaming features like multiplayer games and real-time positioning.

WebSockets

The Problem

HTTP is fundamentally a request-response protocol. The client asks, and the server answers. For real-time updates, you need bidirectional persistent connections where both the client and server can send messages at any time.

How It Works

WebSocket connections begin with an HTTP upgrade request. Here's how the flow works. First, the client sends a standard HTTP GET request, but includes special headers: Upgrade websocket, and Connection Upgrade. This is the handshake.

Second, the server responds with HTTP status code 101 Switching Protocols, along with the Upgrade websocket header. This confirms the protocol switch.

Third, the connection upgrades to a WebSocket connection, which is persistent and bidirectional. Now both the client and server can send messages freely.

Fourth, optionally, the connection can use heartbeat messages. The client sends Ping messages, and the server responds with Pong messages to keep the connection alive.

The WebSocket handshake uses a security mechanism. The Sec-WebSocket-Key header contains a random nonce to prevent cross-protocol attacks. The server responds with Sec-WebSocket-Accept, which is a hash of the key plus a magic string. This proves the server understands the WebSocket protocol.

WebSocket frames have a specific binary format. The FIN bit indicates the final fragment. The opcode field specifies the frame type: 0x1 for text, 0x2 for binary, 0x8 for close, 0x9 for ping, and 0xA for pong. The MASK bit indicates whether the payload is masked, which is required for messages from the client to the server.

Implementation Example

Let me describe how to implement WebSockets using Python with FastAPI. You create a connection manager class that tracks active connections. This manager has methods to connect, disconnect, and broadcast messages.

When a client connects, you accept the websocket connection and add it to your set of active connections. When broadcasting messages, you iterate through all connected clients and send them the message. If any client fails to receive the message, you mark it as disconnected and clean it up afterwards.

The websocket endpoint is an async function that accepts a WebSocket parameter. You connect the websocket using your manager, then enter a loop where you continuously receive text messages from the client. When you receive a message, you broadcast it to all connected clients. If the websocket disconnects, you catch the exception and remove the connection from your manager.

Heartbeat Implementation

To detect dead connections, you implement a heartbeat mechanism. You create an async function that runs in a loop, sleeping for a timeout period like 30 seconds. Then you send a ping message to the websocket. You wait for a pong response with a timeout of 10 seconds. If you don't receive a pong within that time, you know the connection is dead and you close it.

Trade-offs

WebSockets offer very low latency, typically in the millisecond range. They're bidirectional, which is powerful but adds complexity. All modern browsers support WebSockets, though you may encounter firewall issues in some corporate environments.

The main drawback is that persistent connections consume server resources. Each connection uses memory, so you need to plan capacity carefully. Reconnection must be handled manually, which requires complex state management.

When to use WebSockets? They're ideal for chat applications, gaming, collaborative editing, and live dashboards. When not to use them? If you only need simple one-way updates, Server-Sent Events are simpler. If you only need infrequent updates, polling may be sufficient.

Server-Sent Events

The Problem

WebSockets are bidirectional, but often you only need server-to-client updates. Server-Sent Events, or SSE, is a simpler protocol for unidirectional streaming from server to client.

How It Works

With SSE, the client sends a standard HTTP GET request with the Accept header set to text/event-stream. The server responds with HTTP 200 OK and the Content-Type set to text/event-stream. Then the server keeps the connection open and streams events to the client as they occur.

Each SSE message has a specific format. The event field specifies the event type, which is optional and defaults to "message". The id field provides an event ID used for reconnection. The data field contains the event payload, which can span multiple lines.

Implementation

In FastAPI, you create an event generator function. This is an async generator that yields SSE-formatted messages. Each message includes an id line, an event line, and a data line, followed by two newlines to separate messages.

Your SSE endpoint returns a StreamingResponse with your event generator. You set the media type to text/event-stream and include headers for Cache-Control no-cache and Connection keep-alive.

On the client side in JavaScript, you create an EventSource object pointing to your endpoint. You add event listeners for different event types. The EventSource automatically handles reconnection if the connection drops.

Automatic Reconnection

SSE has built-in reconnection support. Your server can send a retry field specifying how many milliseconds the client should wait before reconnecting. For example, "retry: 5000" means reconnect after 5 seconds.

When the client reconnects, it automatically sends a Last-Event-ID header with the ID of the last event it received. Your server can use this to send any missed events.

Trade-offs

SSE is simpler than WebSockets because it's built on standard HTTP. Reconnection is automatic with event ID tracking. All modern browsers support it, though Internet Explorer does not.

However, SSE is unidirectional only - the server can send to the client, but the client must use separate HTTP requests to send data to the server. It's efficient for streaming but still uses a persistent connection.

When to use SSE? It's perfect for live feeds, notifications, stock tickers, and dashboards where you only need server-to-client updates. When not to use it? If you need bidirectional communication, use WebSockets instead. For large binary data, WebSockets are more efficient.

Long Polling

The Problem

Long polling is a fallback technique for environments that block WebSockets or SSE, such as corporate proxies or old browsers that don't support modern protocols.

How It Works

With long polling, the client sends a standard HTTP request to the server. Instead of responding immediately, the server holds the request open, waiting until data becomes available. When data arrives, the server responds with that data. The client immediately sends a new request, starting the cycle again.

Compare this to short polling, which is inefficient. With short polling, the client polls every second. Nine out of ten requests return "No updates", wasting bandwidth and server resources.

With long polling, the server holds the request until an update is available, so you get near real-time updates using standard HTTP.

Implementation

In FastAPI, you create a long poll endpoint that accepts a client ID. You maintain an in-memory queue for each client's pending updates. When a client polls, you wait on their queue with a timeout, say 30 seconds. If an update arrives before the timeout, you return it immediately. If the timeout expires with no update, you return an empty response.

To send messages to clients, you have a broadcast endpoint that puts the message into every client's queue.

On the client side, you have a polling loop that continuously fetches from the poll endpoint. When you receive data, you handle the update and immediately poll again. If there's an error, you wait before retrying to avoid hammering the server.

Trade-offs

Long polling works everywhere because it's just HTTP. It's relatively simple to implement. Latency is near real-time, though higher than WebSockets.

However, it's less efficient than WebSockets or SSE because of frequent reconnections. Each poll opens a new HTTP connection, which has overhead. State management becomes complex because you need to track which clients have pending requests.

When to use long polling? It's a good fallback when WebSocket or SSE aren't available. When not to use it? Don't make it your primary real-time solution. Use WebSockets or SSE as your primary choice and long polling as a fallback.

Scaling WebSocket Connections

The Problem

A single server can only handle about 64,000 concurrent connections because of file descriptor limits. If you have 100,000 users online, you need multiple servers. But this creates a challenge: if User A is connected to Server 1 and User B is connected to Server 2, how do they communicate with each other?

Solution 1: Sticky Sessions

One approach is sticky sessions, where you route the same user to the same server consistently. A load balancer hashes by user ID or cookie to ensure each user always connects to the same backend server.

For example, your load balancer might hash User A and User C to Server 1, User B and User D to Server 2, and User E and User F to Server 3.

In nginx, you configure this with the ip_hash directive in your upstream block. This creates sticky sessions based on the client's IP address. You also need to set the proxy HTTP version to 1.1 and set the Upgrade and Connection headers properly. Set a long proxy read timeout, like 24 hours or 86,400 seconds.

However, sticky sessions alone don't solve the cross-server communication problem. Users on different servers still can't communicate directly. You need a pub/sub backend.

Solution 2: Pub/Sub Backend with Redis

To enable cross-server communication, all servers subscribe to Redis channels. When a message arrives, they broadcast it to their local WebSocket clients.

Here's the flow. User A sends a message to Server 1. Server 1 publishes the message to a Redis channel. Server 2 is subscribed to that channel and receives the message. Server 2 then sends the message to User B over their WebSocket connection.

Implementation with Redis Pub/Sub

You create a PubSubManager class that manages both WebSocket connections and Redis subscriptions. Each connection is organized into rooms.

When a client connects to a room, you add their WebSocket to the room's connection set. To publish a message, you send it to the Redis channel for that room. To subscribe, you create a Redis pub/sub client and listen to the room's channel. When a message arrives on the channel, you send it to all WebSocket clients in that room.

Your websocket endpoint accepts the websocket and room ID. You accept the connection, connect it to the room, and start a subscription task for that room. Then you loop, receiving text from the websocket and publishing it to Redis. All servers subscribed to that room will receive the message and forward it to their clients.

Connection Limits per Server

To prevent overload, you implement a connection limiter. You set a maximum connection count, like 50,000. You use an asyncio Semaphore to control access.

When a client tries to connect, you attempt to acquire a slot from the semaphore. If successful, you increment your connection counter and accept the connection. If you can't acquire a slot, the server is at capacity, so you close the websocket with an appropriate error code.

Always remember to release the semaphore when the connection closes, using a try-finally block to ensure cleanup even if errors occur.

Presence and Typing Indicators

Presence System

A presence system tracks who's online. You create a PresenceManager class that uses Redis with time-to-live keys. To mark a user as online, you set a Redis key with an expiration time, like 5 minutes. To check if a user is online, you check if their key exists. To mark them offline, you delete the key.

For bulk checking, you can pipeline multiple exists commands to Redis for efficiency.

The key insight is using Redis expiration. If a user disconnects ungracefully without sending an offline message, their presence key automatically expires after the timeout, so they appear offline.

You implement a heartbeat mechanism where the client periodically renews their presence by calling mark_online again, which resets the TTL.

Typing Indicators

Typing indicators show "User is typing..." in chat applications. You create a TypingManager similar to the presence manager. When a user starts typing, you set a Redis key with a short timeout, like 5 seconds. When they stop typing, you delete the key. To get all users typing in a room, you query Redis for all keys matching the room's pattern.

The client sends typing events through the WebSocket. When they start typing, you mark them as typing in Redis and broadcast a "user_typing" event to all users in the room. When they stop typing or send a message, you mark them as stopped typing and broadcast a "user_stopped_typing" event.

The 5-second timeout is important. If a user starts typing but never sends a stop event, maybe because their connection dropped, the typing indicator automatically disappears after 5 seconds.

Reconnection Strategies

Exponential Backoff

On the client side, you implement reconnection with exponential backoff. You start with a short reconnect delay, like 1 second. Each time reconnection fails, you double the delay up to a maximum, like 30 seconds.

When the websocket opens successfully, you reset the delay back to the initial 1 second. When it closes, you schedule a reconnection after the current delay, then double the delay for the next attempt. When an error occurs, you log it for debugging.

Before sending data, you check if the websocket is in the OPEN state. If not, you queue the message or warn that the socket isn't ready.

Session Recovery

To restore state after reconnection, you implement session management. Each connection gets a session ID. You track the last message ID the client received.

When a client reconnects, they send their session ID and last message ID. Your server queries for messages newer than that ID and sends them to the client, so they don't miss anything.

For new connections, you create a new session and send the session ID to the client for future reconnections.

Comparison of Real-time Technologies

Let me compare the main real-time technologies across several dimensions.

Direction: WebSocket is bidirectional. SSE is server-to-client only. Long Polling is request-response. gRPC Streaming is bidirectional.

Protocol: WebSocket uses its own WebSocket protocol. SSE and Long Polling use HTTP. gRPC Streaming uses HTTP/2.

Browser support: WebSocket has excellent support. SSE is good but not supported in Internet Explorer. Long Polling is universal. gRPC Streaming has limited browser support.

Latency: WebSocket and gRPC Streaming have very low latency. SSE has low latency. Long Polling has medium latency.

Reconnection: SSE has automatic reconnection. WebSocket and gRPC Streaming require manual handling. Long Polling doesn't need reconnection since each request is independent.

Binary data: WebSocket, Long Polling, and gRPC Streaming support binary. SSE does not.

Firewall friendliness: SSE and Long Polling are firewall friendly. WebSocket and gRPC Streaming are sometimes blocked.

Complexity: SSE is low complexity. WebSocket and Long Polling are medium. gRPC Streaming is high complexity.

Best use cases: WebSocket is best for chat and gaming. SSE is best for live feeds and notifications. Long Polling is best as a fallback. gRPC Streaming is best for service-to-service communication.

Key Concepts Checklist

Let me summarize the key concepts you should understand. First, understand the WebSocket handshake process and frame format. Second, know when to use WebSocket versus SSE versus Long Polling based on your requirements. Third, implement heartbeat or ping-pong mechanisms for connection health monitoring. Fourth, design a reconnection strategy with exponential backoff to handle network failures gracefully. Fifth, scale WebSockets across multiple servers using Redis Pub/Sub for cross-server messaging. Sixth, implement a presence system using Redis with time-to-live keys. Seventh, handle typing indicators with expiring Redis keys. Eighth, configure sticky sessions for connection affinity to route users to the same server.

Practical Insights

Let me share some practical insights from running real-time systems in production.

Connection limits are a critical concern. Linux has a default limit of 1024 file descriptors per process. You need to increase this with ulimit -n 65535 or configure systemd with LimitNOFILE=65535. A rule of thumb is that 50,000 connections per server is realistic. Monitor your open file descriptors to ensure you don't hit limits.

Memory per connection is another important consideration. Each WebSocket connection uses TCP buffers of 16 to 64 kilobytes for send and receive. Application buffers use 8 to 32 kilobytes. Python object overhead adds about 5 kilobytes. In total, each connection uses roughly 50 to 100 kilobytes. With 50,000 connections, you're looking at 2.5 to 5 gigabytes of RAM just for connections.

Heartbeat tuning requires balance. If heartbeats are too frequent, you waste bandwidth. If too infrequent, you're slow to detect dead connections. The sweet spot is typically a 30 to 60 second ping interval with a 10 second pong timeout. Note that load balancers often have 60 to 300 second idle timeouts, so your heartbeat should be more frequent than that.

Redis Pub/Sub at scale requires sharding. The problem is that all servers subscribe to all channels. With 1000 rooms and 10 servers, that's 10,000 subscriptions. The solution is to shard channels across multiple Redis instances. You hash the room ID modulo the number of Redis instances to determine which instance handles that room. Now 1000 rooms divided by 5 Redis shards means only 200 rooms per Redis instance.

Binary versus text frames is a choice you need to make. Text frames are UTF-8 encoded with larger overhead. Binary frames are raw bytes, which is efficient for large data. Use binary for images, video, audio, and protobuf messages. Use text for JSON messages and simple commands.

Graceful shutdown is important during deployments. Don't abruptly close connections when deploying new code. Instead, send a close frame to the client indicating a server restart. Wait for the client to acknowledge or timeout after a few seconds. Then close the connection gracefully. This gives clients time to reconnect smoothly instead of seeing abrupt disconnections.

This concludes Chapter 17 on Real-time Systems. You should now understand the different approaches to real-time communication, when to use each one, and how to scale them in production environments.
